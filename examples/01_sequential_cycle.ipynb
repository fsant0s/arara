{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.abspath('../..')\n",
    "sys.path.append(ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging session ID: e9d88436-3fc0-4e17-8f34-7fffcfd36282\n"
     ]
    }
   ],
   "source": [
    "from neuron.neurons import Neuron, User\n",
    "from neuron.neurons import Neuron\n",
    "from neuron.capabilities import DataFrameRetrieverCapability, RerankCapability\n",
    "from neuron.components import SequentialComponent, CycleComponent, Pipeline\n",
    "\n",
    "# Start logging\n",
    "from neuron.runtime_logging import start\n",
    "logging_session_id = start(config={\"dbname\": \"logs.db\"})\n",
    "print(\"Logging session ID: \" + str(logging_session_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(r\"./data/yelp_academic_dataset_business.json\", lines=True, orient='columns', chunksize=1000000)\n",
    "# read the data\n",
    "for business in df:\n",
    "    business = business.head(100)\n",
    "    break\n",
    "\n",
    "business.drop(['business_id', 'is_open', 'hours', 'longitude', 'latitude', 'postal_code', 'state', 'city', 'attributes'], axis=1, inplace=True)\n",
    "business.rename(columns={'name': 'place_name'}, inplace=True)\n",
    "\n",
    "business['categories'] = business['categories'].str.split(', ')\n",
    "business = business.explode('categories', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_name</th>\n",
       "      <th>address</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Range At Lake Norman</td>\n",
       "      <td>10913 Bailey Rd</td>\n",
       "      <td>3.5</td>\n",
       "      <td>36</td>\n",
       "      <td>Active Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Range At Lake Norman</td>\n",
       "      <td>10913 Bailey Rd</td>\n",
       "      <td>3.5</td>\n",
       "      <td>36</td>\n",
       "      <td>Gun/Rifle Ranges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Range At Lake Norman</td>\n",
       "      <td>10913 Bailey Rd</td>\n",
       "      <td>3.5</td>\n",
       "      <td>36</td>\n",
       "      <td>Guns &amp; Ammo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Range At Lake Norman</td>\n",
       "      <td>10913 Bailey Rd</td>\n",
       "      <td>3.5</td>\n",
       "      <td>36</td>\n",
       "      <td>Shopping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carlos Santo, NMD</td>\n",
       "      <td>8880 E Via Linda, Ste 107</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Health &amp; Medical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 place_name                    address  stars  review_count  \\\n",
       "0  The Range At Lake Norman            10913 Bailey Rd    3.5            36   \n",
       "1  The Range At Lake Norman            10913 Bailey Rd    3.5            36   \n",
       "2  The Range At Lake Norman            10913 Bailey Rd    3.5            36   \n",
       "3  The Range At Lake Norman            10913 Bailey Rd    3.5            36   \n",
       "4         Carlos Santo, NMD  8880 E Via Linda, Ste 107    5.0             4   \n",
       "\n",
       "         categories  \n",
       "0       Active Life  \n",
       "1  Gun/Rifle Ranges  \n",
       "2       Guns & Ammo  \n",
       "3          Shopping  \n",
       "4  Health & Medical  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\capabilities\\data_frame_retriever_capability.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "llm_config={\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"client\": \"groq\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"model\": \"llama-3.3-70b-versatile\",\n",
    "            \"api_key\": os.getenv(\"GROQ_API_KEY\")\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "user = User(\n",
    "    name=\"User\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "extract_info_neuron = Neuron(\n",
    "    name=\"extract_info_neuron\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "    You are an AI assistant tasked with extracting specific features from a user request. Your role is to identify and include only the features that are explicitly mentioned or can be directly inferred from the provided text.\n",
    "\n",
    "    ### Features to Extract:\n",
    "    1. 'stars' (ratings) as a numeric value.\n",
    "    2. 'review_count' as a numeric value.\n",
    "    3. 'categories' describing the business type. If not explicitly mentioned, infer from context or exclude it if unclear.\n",
    "\n",
    "    ### Output Format:\n",
    "    Include only the features that are explicitly mentioned or inferable, formatted as:\n",
    "    - stars is {stars_value}\n",
    "    - review_count is {review_count_value}\n",
    "    - categories is {categories_value}\n",
    "\n",
    "    ### Rules:\n",
    "    1. Include only the features described or inferable in the input. If a feature is not mentioned or cannot be inferred, do not include it in the output.\n",
    "    2. Provide only the extracted details. Do not leave placeholders or add features with empty values.\n",
    "    3. Do not include comments, explanations, or extra text in the output.\n",
    "\n",
    "    ### Examples:\n",
    "    1. Input: \"I want to eat pizza at a restaurant with at least 3 stars and more than 30 reviews.\"\n",
    "    Output:\n",
    "    stars is 3\n",
    "    review_count is 30\n",
    "    categories is Restaurant\n",
    "\n",
    "    2. Input: \"I need a clinic with at least 4 stars.\"\n",
    "    Output:\n",
    "    stars is 4\n",
    "    categories is Health & Medical\n",
    "\n",
    "    3. Input: \"Looking for a nice park with good views.\"\n",
    "    Output:\n",
    "    categories is Park\n",
    "\n",
    "    4. Input: \"I need something fun to do.\"\n",
    "    Output:\n",
    "    (No output)\n",
    "    \"\"\",\n",
    "    shared_memory_write_keys = [\"extract_info\"],\n",
    ")\n",
    "\n",
    "retriever_neuron = Neuron(\n",
    "    name=\"retriever_neuron\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "    Format the retrieved items into a Python list of strings. Each string should include all item details in the format:\n",
    "\n",
    "    [\n",
    "        'Item 1: address is 123 Main St, categories is Shopping, place_name is Example Store, review_count is 10, stars is 4.0',\n",
    "        'Item 2: address is 456 Elm St, categories is Food, place_name is Example Cafe, review_count is 15, stars is 4.5'\n",
    "    ]\n",
    "\n",
    "    Output only the Python list of strings with all item details.\n",
    "    Remove any additional text, comments, or information. I mean, leave only the list of strings.\n",
    "    \"\"\"\n",
    ")\n",
    "data_frame_retriever_capability = DataFrameRetrieverCapability(\n",
    "    dataset=business,\n",
    "    columns = [\"stars\" , \"review_count\", \"categories\"],\n",
    "    cache=True,\n",
    "    top_n=10,\n",
    "    config= {\n",
    "        \"model\": \"gpt2-medium\",\n",
    "        \"model_dir\": r\"./models/erasmo_yelp_gpt2-medium_60_True\"\n",
    "    },\n",
    ")\n",
    "\n",
    "data_frame_retriever_capability.add_to_neuron(retriever_neuron)\n",
    "\n",
    "rerank_neuron = Neuron(\n",
    "    name=\"rerank_neuron\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "        You will be provided a list of item which were reranked.\n",
    "        Your goal is to create a short introduction about the list to the user.\n",
    "        Also, include all items in the list in the output.\n",
    "    \"\"\",\n",
    ")\n",
    "rerank_capability = RerankCapability(\n",
    "    rerank_model_name = \"rerank-english-v2.0\", #\"rerank-v3.5\",\n",
    "    api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "    top_n=10,\n",
    "    shared_memory_read_keys=[\"extract_info\"],\n",
    ")\n",
    "rerank_capability.add_to_neuron(rerank_neuron)\n",
    "\n",
    "assessor_neuron = Neuron(\n",
    "    name=\"assessor_neuron\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "You are an Assessor assistant tasked with evaluating whether each retrieved item matches the user's specified criteria for `stars`, `review count`, and `category`.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Understand the User's Criteria:\n",
    "   - Identify the minimum requirements for `stars`, `review count`, and `category`.\n",
    "\n",
    "2. Evaluate Each Item:\n",
    "   - For each retrieved item, check:\n",
    "     - Does the `stars` value meet or exceed the user's requirement?\n",
    "     - Does the `review count` value meet or exceed the user's requirement?\n",
    "     - Does the `category` match the user's specified category?\n",
    "   - Respond for each item with:\n",
    "     - The item itself, followed by `\"Matches the user's criteria.\"` if it meets all requirements.\n",
    "     - The item itself, followed by `\"Does not match the user's criteria.\"` if it fails any requirement.\n",
    "\n",
    "3. Output:\n",
    "   - List each retrieved item alongside its evaluation in the format:\n",
    "     ```plaintext\n",
    "     Item: <item_details> - Matches the user's criteria.\n",
    "     Item: <item_details> - Does not match the user's criteria.\n",
    "     ```\n",
    "   - Ensure all items are included in the output.\n",
    "    \"\"\",\n",
    "    shared_memory_read_keys = [\"extract_info\"],\n",
    "    shared_memory_transition_message = [\"Below is the user's request:\"]\n",
    ")\n",
    "\n",
    "recommender = Neuron(\n",
    "    name=\"recommender\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "    As a Recommender assistant, your task is to process the Assessor's item-by-item evaluation and return only the items that meet the user’s criteria.\n",
    "\n",
    "    ### Guidelines:\n",
    "\n",
    "    1. Understand the Inputs:\n",
    "    - You will receive the user's request and a list of item evaluations from the Assessor.\n",
    "\n",
    "    2. Filter Based on the Assessor's Evaluation:\n",
    "    - For each item evaluated as `\"Matches the user's criteria\"` by the Assessor, include it in your output.\n",
    "    - Exclude all items evaluated as `\"Does not match the user's criteria\"`.\n",
    "\n",
    "    3. Output the Matching Items:\n",
    "    - Return the exact details of the matching items, including all their attributes (e.g., name, location, stars, review count, category).\n",
    "    - If no items match, return a concise message:\n",
    "        `\"No items match the user’s request.\"`\n",
    "\n",
    "    4. Be Precise and Minimalistic:\n",
    "    - Ensure the response contains only the matching items or the short message if no items match.\n",
    "    - Do not include any additional comments or explanations.\n",
    "    \"\"\",\n",
    "    shared_memory_read_keys=[\"extract_info\"],\n",
    "    shared_memory_transition_message=[\"Below is the user's request:\"],\n",
    "    shared_memory_write_keys=[\"recommend_items\"]\n",
    ")\n",
    "\n",
    "exaplainer = Neuron(\n",
    "    name=\"exaplainer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "    You are an Explainer Assistant tasked with providing a clear explanation for the items recommended by the Recommender assistant based on the user's request. Your role is to justify the relevance of recommended items or explain why no items were recommended.\n",
    "\n",
    "    Inputs:\n",
    "    1. User Request: A description specifying the user's preferences or criteria.\n",
    "    2. Recommender Response: A list of recommended items or an indication that no items matched.\n",
    "\n",
    "    Guidelines:\n",
    "\n",
    "    1. Recommended Items:\n",
    "    - Begin with: \"For the recommended items, here is an explanation:\"\n",
    "    - For each item, provide:\n",
    "    - Item Name: [Name]\n",
    "    - Explanation: [How it meets the user's criteria, emphasizing key attributes like stars, review count, and category.]\n",
    "\n",
    "    2. No Recommendations:\n",
    "    - State: \"No items were recommended based on the user's request.\"\n",
    "    - Follow with: \"Here are the unmet criteria:\" and list the specific criteria not satisfied.\n",
    "\n",
    "    3. Output Format:\n",
    "    - For Recommended Items:\n",
    "    For the recommended items, here is an explanation:\n",
    "    Item Name: [Name]\n",
    "    Explanation: [Details about how it meets the user's request.]\n",
    "\n",
    "    - For No Recommendations:\n",
    "    No items were recommended based on the user's request.\n",
    "    Here are the unmet criteria:\n",
    "    - [Criterion 1]\n",
    "    - [Criterion 2]\n",
    "\n",
    "    Example Outputs:\n",
    "\n",
    "    Case 1: Recommended Items\n",
    "    For the recommended items, here is an explanation:\n",
    "    Item Name: \"Wireless Headphones\"\n",
    "    Explanation: \"Meets the criteria of 4.5 stars or higher, over 100 reviews, and the 'Electronics' category.\"\n",
    "\n",
    "    Item Name: \"Bluetooth Earbuds\"\n",
    "    Explanation: \"Satisfies the request for portability, high ratings, and the specified 'Electronics' category.\"\n",
    "\n",
    "    Case 2: No Recommendations\n",
    "    No items were recommended based on the user's request.\n",
    "    Here are the unmet criteria:\n",
    "    - Minimum rating of 4.0 stars.\n",
    "    - Category: 'Home Appliances'.\n",
    "\n",
    "    Tone:\n",
    "    - Be clear, concise, and focused on the user's criteria.\n",
    "    - Avoid redundant or overly technical language.\n",
    "    \"\"\",\n",
    "    shared_memory_read_keys=[\"extract_info\"],\n",
    "    shared_memory_transition_message=[\"Below is the user's request:\"],\n",
    ")\n",
    "\n",
    "evaluator = Neuron(\n",
    "    name=\"evaluator\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "   You are an Evaluator Assistant tasked with validating the consistency of the process, which includes the user’s request, recommended items, and the Explainer Assistant explanation. Your goal is to ensure the recommendations and explanations align with the user’s criteria.\n",
    "\n",
    "    Inputs:\n",
    "    1. User Request: The user’s criteria (e.g., stars, review count, category). Only consider explicitly mentioned features.\n",
    "    2. Recommended Items: The items provided by the Recommender assistant.\n",
    "    3. Explanation: The justification provided by the Explainer assistant.\n",
    "\n",
    "    Guidelines:\n",
    "\n",
    "    1. Validate Recommendations:\n",
    "    - Check if all recommended items meet the user’s explicitly described criteria.\n",
    "    - Disregard features not mentioned in the user’s request.\n",
    "\n",
    "    2. Verify Explanation:\n",
    "    - Confirm that the Explainer assistant explanation accurately reflects the attributes of the recommended items and aligns with the user’s request.\n",
    "    - Identify any mismatches between the explanation and the recommendations.\n",
    "\n",
    "    3. Output:\n",
    "    - If the process is consistent:\n",
    "    - Respond: \"The process is consistent. Here are the validated recommendations and explanations:\"\n",
    "    - List the recommended items with their attributes and the corresponding explanation.\n",
    "    - If the process is inconsistent:\n",
    "    - Respond: \"The evaluation failed due to the following issues:\"\n",
    "    - Specify:\n",
    "        - Items that do not meet the user’s criteria.\n",
    "        - Mismatches between the explanation and the recommendations or user’s request.\n",
    "\n",
    "    Example Outputs:\n",
    "\n",
    "    Case 1: Process is Consistent\n",
    "    \"The process is consistent. Here are the validated recommendations and explanations:\"\n",
    "    - Item: \"Wireless Headphones\"\n",
    "    Attributes: 4.5 stars, 150 reviews, Category: Electronics\n",
    "    Explanation: \"Meets the criteria of 4.5 stars or higher, over 100 reviews, and belongs to the requested category 'Electronics'.\"\n",
    "\n",
    "    - Item: \"Bluetooth Earbuds\"\n",
    "    Attributes: 4.8 stars, 200 reviews, Category: Electronics\n",
    "    Explanation: \"Satisfies the request for portability, high ratings, and the specified category 'Electronics'.\"\n",
    "\n",
    "    Case 2: Evaluation Failed\n",
    "    \"The evaluation failed due to the following issues:\"\n",
    "    - Item \"Bluetooth Earbuds\" does not meet the minimum review count.\n",
    "    - The explanation incorrectly states that all items meet the review count criteria.\n",
    "\n",
    "    Tone:\n",
    "    - Be direct and objective.\n",
    "    - Focus on clarity and relevance to the user’s criteria.\n",
    "    \"\"\",\n",
    "    shared_memory_read_keys=[\"extract_info\", \"recommend_items\"],\n",
    "    shared_memory_transition_message=[\"Below is the user's request:\", \"Items recommended:\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_1 = CycleComponent(name=\"Cycle_1\", neurons=[extract_info_neuron, retriever_neuron, rerank_neuron, assessor_neuron], repetitions=1)\n",
    "sequential2 = SequentialComponent(name=\"Seq_1\", neurons=[recommender, exaplainer, evaluator])\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_edge(cycle_1, sequential2)\n",
    "pipeline.set_entry_point(cycle_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to extract_info_neuron):\n",
      "\n",
      "Shopping outlet with at least 3 stars and more than 4 reviews.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Groq exception occurred: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\clients\\cloud_based\\groq.py:103\u001b[0m, in \u001b[0;36mGroqClient.create\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgroq_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:287\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03mCreates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1244\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1241\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1242\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1243\u001b[0m )\n\u001b[1;32m-> 1244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\.venv\\Lib\\site-packages\\groq\\_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1039\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1042\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1043\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[0;32m   1048\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43muser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#pipeline, message=\"I need a Health & Medical clinic with at least 1 stars and more than 5 reviews.\",\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShopping outlet with at least 3 stars and more than 4 reviews.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\neurons\\neuron.py:155\u001b[0m, in \u001b[0;36mNeuron.initiate_chat\u001b[1;34m(self, recipient, should_clear_history, silent, message, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m prepare_chat(\u001b[38;5;28mself\u001b[39m, recipient, should_clear_history)\n\u001b[0;32m    154\u001b[0m msg2send \u001b[38;5;241m=\u001b[39m message \n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\neurons\\neuron.py:167\u001b[0m, in \u001b[0;36mNeuron.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    165\u001b[0m valid \u001b[38;5;241m=\u001b[39m append_oai_message(\u001b[38;5;28mself\u001b[39m, message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\neurons\\neuron.py:183\u001b[0m, in \u001b[0;36mNeuron.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\neurons\\neuron.py:211\u001b[0m, in \u001b[0;36mNeuron.generate_reply\u001b[1;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m reply_func \u001b[38;5;241m=\u001b[39m reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match_trigger(\u001b[38;5;28mself\u001b[39m, reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[1;32m--> 211\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled() \u001b[38;5;129;01mand\u001b[39;00m final \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m         log_event(\n\u001b[0;32m    214\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    215\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[0;32m    220\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\components\\pipeline.py:86\u001b[0m, in \u001b[0;36mPipeline.execute\u001b[1;34m(self, messages, sender, config, reply_to_user)\u001b[0m\n\u001b[0;32m     83\u001b[0m user \u001b[38;5;241m=\u001b[39m sender\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_component:\n\u001b[1;32m---> 86\u001b[0m     neuron, message, default_component\u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     sender \u001b[38;5;241m=\u001b[39m neuron\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default_component \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\components\\cycle.py:51\u001b[0m, in \u001b[0;36mCycleComponent.execute\u001b[1;34m(self, sender, message, silent)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons):\n\u001b[0;32m     50\u001b[0m     speaker\u001b[38;5;241m.\u001b[39msend(message, neuron, request_reply\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 51\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m neuron  \u001b[38;5;66;03m# Update speaker to the current neuron\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     message \u001b[38;5;241m=\u001b[39m response  \u001b[38;5;66;03m# Update message to the response for the next cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\neurons\\neuron.py:211\u001b[0m, in \u001b[0;36mNeuron.generate_reply\u001b[1;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m reply_func \u001b[38;5;241m=\u001b[39m reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match_trigger(\u001b[38;5;28mself\u001b[39m, reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[1;32m--> 211\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled() \u001b[38;5;129;01mand\u001b[39;00m final \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m         log_event(\n\u001b[0;32m    214\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    215\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[0;32m    220\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\neurons\\neuron.py:238\u001b[0m, in \u001b[0;36mNeuron._generate_oai_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m--> 238\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\neurons\\neuron.py:253\u001b[0m, in \u001b[0;36mNeuron._generate_oai_reply_from_client\u001b[1;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m    251\u001b[0m     all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m--> 253\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[0;32m    255\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\clients\\client_wrapper.py:261\u001b[0m, in \u001b[0;36mClientWrapper.create\u001b[1;34m(self, **config)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[1;32m--> 261\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    263\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\neuron\\clients\\cloud_based\\groq.py:105\u001b[0m, in \u001b[0;36mGroqClient.create\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    103\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgroq_params)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroq exception occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Groq exception occurred: Error code: 400 - {'error': {'message': 'The model `llama3-groq-70b-8192-tool-use-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "user.initiate_chat(\n",
    "    #pipeline, message=\"I need a Health & Medical clinic with at least 1 stars and more than 5 reviews.\",\n",
    "    pipeline, message=\"Shopping outlet with at least 3 stars and more than 4 reviews.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.abspath('../..')\n",
    "sys.path.append(ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "from neuron.runtime_logging import start\n",
    "logging_session_id = start(config={\"dbname\": \"logs.db\"})\n",
    "print(\"Logging session ID: \" + str(logging_session_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron.components import SequentialComponent, CycleComponent, Pipeline\n",
    "from neuron.neurons import User, Neuron, RouterNeuron\n",
    "\n",
    "llm_config={\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"client\": \"groq\",\n",
    "            \"model\": \"llama3-groq-70b-8192-tool-use-preview\",\n",
    "            \"api_key\": \"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\",\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "user = User(name=\"User\", llm_config=llm_config)\n",
    "\n",
    "common_seq1 = \"Dado um numero fornecido pelo usuario, vc deve adicionar mais um a esse numero. Seu output é somente o numero somado, sem texto adicional. Considere a ultima mensagem do user.\"\n",
    "common_seq2 = \"Dado um numero fornecido pelo usuario, vc deve adicionar mais dois a esse numero. Seu output é somente o numero somado, sem texto adicional. Considere a ultima mensagem do user.\"\n",
    "\n",
    "a1 = Neuron(name=\"Assistante 1\", llm_config=llm_config, system_message=common_seq1)\n",
    "seq1 = SequentialComponent(name=\"Sequential 1\", neurons=[a1])\n",
    "\n",
    "a0 = Neuron(name=\"Assistante 0\", llm_config=llm_config, system_message=\"Multiplica o numero fornecido pelo usuario por 2.\")\n",
    "seq0 = SequentialComponent(name=\"Sequential 0\", neurons=[a0])\n",
    "\n",
    "a2 = Neuron(name=\"Assistante 2\", llm_config=llm_config, system_message=common_seq1)\n",
    "a3 = Neuron(name=\"Assistante 3\", llm_config=llm_config, system_message=common_seq1)\n",
    "a4 = Neuron(name=\"Assistante 4\", llm_config=llm_config, system_message=common_seq1)\n",
    "\n",
    "cycle_router_neuron = Neuron(\n",
    "    name=\"Cycle Router\",\n",
    "    llm_config=llm_config,\n",
    "    system_message = \"\"\"\n",
    "    \"Evaluate the input based on the number provided by the user. Extract the number if it's part of a larger text. If the number is greater than 10, respond with 'TERMINATE' exactly, with no additional text or explanation. If the number is 10 or less, or if no number is provided, do not respond at all. For example, if the input is 'The number is 15,' reply 'TERMINATE.' If the input is 'The number is 5,' provide no response.\n",
    "    \"\"\",\n",
    ")\n",
    "cycle = CycleComponent(name=\"Cycle 1\", neurons=[a2, a3, a4],\n",
    "                       repetitions=1,\n",
    "                       default_component = seq0,\n",
    "                       cycle_router_neuron= cycle_router_neuron)\n",
    "\n",
    "\n",
    "a5 = Neuron(name=\"Assistante 5\", llm_config=llm_config, system_message=common_seq1)\n",
    "seq2 = SequentialComponent(name=\"Sequential 2\", neurons=[a5])\n",
    "\n",
    "a6 = Neuron(name=\"Assistante 6\", llm_config=llm_config, system_message=common_seq2)\n",
    "a7 = Neuron(name=\"Assistante 7\", llm_config=llm_config, system_message=common_seq2)\n",
    "seq3 = SequentialComponent(name=\"Sequential 3\", neurons=[a6, a7])\n",
    "\n",
    "\n",
    "router1 = RouterNeuron(\n",
    "    llm_config=llm_config,\n",
    "    name=\"router_1\",\n",
    "    route_mapping_function = lambda: {\"0\": \"continue\", \"1\": seq2, \"2\": seq3},\n",
    "    system_message = \"If the number is even, return '1'. If the number is odd, return '2'. Otherwise, return 'continue'.\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_edge(seq1, cycle)\n",
    "pipeline.add_edge(cycle, router1)\n",
    "pipeline.set_entry_point(seq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.initiate_chat(\n",
    "    pipeline, message=\"Comece de 15\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
