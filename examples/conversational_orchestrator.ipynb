{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "ROOT_DIR = os.path.abspath('..')\n",
    "sys.path.append(ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in CreateResult has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\neuron\\.venv\\Lib\\site-packages\\accelerate\\utils\\other.py:220: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  np.core.multiarray._reconstruct,\n"
     ]
    }
   ],
   "source": [
    "from neuron.neurons import User, Neuron, ConversationalOrchestrator, ConversationalOrchestratorManager, UserTest\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"client\": \"groq\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"model\": \"gemma2-9b-it\",\n",
    "            \"api_key\": os.getenv(\"GROQ_API_KEY\")\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "user = User(\n",
    "    description=\"You are a human user interacting with conversational and recommendation agents.\"\n",
    ")\n",
    "\n",
    "conversational = Neuron(\n",
    "    name=\"conversational\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"\n",
    "    \"You are an expert conversational agent skilled at natural interactions. Whenever the user requests recommendations or suggestions, immediately involve the recommender agent.\"\"\",\n",
    "    description=\"This agent acts as a social companion and facilitates interactions.\"\n",
    ")\n",
    "\n",
    "llm_config_2 = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"client\": \"groq\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"model\": \"llama-3.3-70b-versatile\",\n",
    "            \"api_key\": os.getenv(\"GROQ_API_KEY\")\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "recommendation = Neuron(\n",
    "    name=\"recommender\",\n",
    "    llm_config=llm_config_2,\n",
    "    system_message=\"\"\"\n",
    "        You are an expert recommender agent. Whenever activated, you should clearly suggest the following items:\n",
    "            1. Shoes\n",
    "            2. T-shirts\n",
    "            3. Jeans\n",
    "Provide these suggestions directly when requested by the conversational agent or when the user explicitly asks for recommendations.\"\"\",\n",
    "    description=\"This agent specializes in offering product recommendations.\"\n",
    ")\n",
    "\n",
    "chitchat = ConversationalOrchestrator(\n",
    "    agents=[user, conversational, recommendation],\n",
    "    messages=[],\n",
    "    max_round=100,\n",
    "    speaker_selection_method=\"random\",\n",
    "    send_introductions=True,\n",
    "    allow_repeat_speaker = False,\n",
    ")\n",
    "\n",
    "chitchat_manager = ConversationalOrchestratorManager(chitchat=chitchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message source='user' target='conversational_orchestrator' models_usage=None metadata={} content='Oi tudo bem?' type='TextMessage'\n",
      "\u001b[1m\u001b[34m⟶ [user → conversational_orchestrator]:\u001b[0m\n",
      "Oi tudo bem?\n",
      "message Response(chat_message=TextMessage(source='conversational_orchestrator', target=None, models_usage=RequestUsage(prompt_tokens=82, completion_tokens=14, total_tokens=96), metadata={}, content='\\n\\nPlease let us know how we can help you!\\n', type='TextMessage'), inner_messages=[])\n",
      "\u001b[1m\u001b[34m⟶ [conversational_orchestrator]:\u001b[0m\n",
      "\n",
      "\n",
      "Please let us know how we can help you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_result = user.initiate_chat(\n",
    "    chitchat_manager, message=\"Oi tudo bem?\", summary_method=\"reflection_with_llm\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
